{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work\n",
    "1. 請改變 reduce_lr 的 patience 和 factor 並比較不同設定下，對訓練/驗證集的影響\n",
    "2. 請將 optimizer 換成 Adam、RMSprop 搭配 reduce_lr 並比較訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Users\\yehch\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Users\\yehch\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Users\\yehch\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Users\\yehch\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Users\\yehch\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Users\\yehch\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "\n",
    "# Disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 資料前處理\n",
    "def preproc_x(x, flatten=True):\n",
    "    x = x / 255.\n",
    "    if flatten:\n",
    "        x = x.reshape((len(x), -1))\n",
    "    return x\n",
    "\n",
    "def preproc_y(y, num_classes=10):\n",
    "    if y.shape[-1] == 1:\n",
    "        y = keras.utils.to_categorical(y, num_classes)\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train\n",
    "x_test, y_test = test\n",
    "\n",
    "data_ratio = 20\n",
    "# 將資料集減少為1/20\n",
    "# 資料前處理 - X 標準化\n",
    "x_train = preproc_x(x_train[::data_ratio])\n",
    "x_test = preproc_x(x_test[::data_ratio])\n",
    "\n",
    "# 資料前處理 -Y 轉成 onehot\n",
    "y_train = preproc_y(y_train[::data_ratio])\n",
    "y_test = preproc_y(y_test[::data_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "def build_mlp(input_shape, output_units=10, num_neurons=[256, 256, 256]):\n",
    "    \"\"\"Code Here\n",
    "    建立你的神經網路\n",
    "    \"\"\"\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "    \n",
    "    for i, n_units in enumerate(num_neurons):\n",
    "        if i == 0:\n",
    "            x = keras.layers.Dense(units=n_units, \n",
    "                                   activation=\"relu\", \n",
    "                                   name=\"hidden_layer\"+str(i+1))(input_layer)\n",
    "            x = BatchNormalization()(x)\n",
    "        else:\n",
    "            x = keras.layers.Dense(units=n_units, \n",
    "                                   activation=\"relu\", \n",
    "                                   name=\"hidden_layer\"+str(i+1))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "    \n",
    "    out = keras.layers.Dense(units=output_units, activation=\"softmax\", name=\"output\")(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[input_layer], outputs=[out])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 25 # IF you feel too run to finish, try to make it smaller\n",
    "BATCH_SIZE = 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 Callbacks\n",
    "'''\n",
    "請改變 reduce_lr 的 patience 和 factor 並比較不同設定下，對訓練/驗證集的影響\n",
    "請將 optimizer 換成 Adam、RMSprop 搭配 reduce_lr 並比較訓練結果\n",
    "'''\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "optimizer_set = [keras.optimizers.SGD(lr=LEARNING_RATE, nesterov=True, momentum=0.95),\n",
    "                 keras.optimizers.Adam(lr=LEARNING_RATE),\n",
    "                 keras.optimizers.RMSprop(lr=LEARNING_RATE)]\n",
    "\n",
    "\"\"\"Code Here\n",
    "建立實驗的比較組合\n",
    "\"\"\"\n",
    "reduce_lr_factor = [0.2, 0.5]\n",
    "redice_lr_patient = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of exp: 0, reduce_factor: 0.20, reduce_patient: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 3.0276 - acc: 0.1040 - val_loss: 2.9201 - val_acc: 0.1340\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 2.7491 - acc: 0.1376 - val_loss: 2.9337 - val_acc: 0.1640\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.4872 - acc: 0.1840 - val_loss: 3.0514 - val_acc: 0.1740\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 2.2793 - acc: 0.2384 - val_loss: 2.9943 - val_acc: 0.1760\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 2.1214 - acc: 0.2812 - val_loss: 2.9420 - val_acc: 0.1940\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 2.0349 - acc: 0.3096 - val_loss: 2.8885 - val_acc: 0.1940\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.9760 - acc: 0.3272 - val_loss: 2.8256 - val_acc: 0.2040\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 1.9277 - acc: 0.3416 - val_loss: 2.7529 - val_acc: 0.2160\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 1.8889 - acc: 0.3484 - val_loss: 2.6854 - val_acc: 0.2220\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 1.8472 - acc: 0.3640 - val_loss: 2.6250 - val_acc: 0.2360\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 1.8086 - acc: 0.3804 - val_loss: 2.5640 - val_acc: 0.2380\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 1.7777 - acc: 0.3804 - val_loss: 2.5092 - val_acc: 0.2440\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 88us/step - loss: 1.7456 - acc: 0.3952 - val_loss: 2.4633 - val_acc: 0.2540\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 1.7132 - acc: 0.4028 - val_loss: 2.4240 - val_acc: 0.2540\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 1.6898 - acc: 0.4160 - val_loss: 2.3910 - val_acc: 0.2580\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 1.6642 - acc: 0.4228 - val_loss: 2.3646 - val_acc: 0.2520\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 1.6335 - acc: 0.4336 - val_loss: 2.3422 - val_acc: 0.2520\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.6108 - acc: 0.4408 - val_loss: 2.3225 - val_acc: 0.2560\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 1.5868 - acc: 0.4476 - val_loss: 2.3046 - val_acc: 0.2660\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 105us/step - loss: 1.5674 - acc: 0.4556 - val_loss: 2.2883 - val_acc: 0.2700\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 1.5430 - acc: 0.4604 - val_loss: 2.2743 - val_acc: 0.2740\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.5248 - acc: 0.4684 - val_loss: 2.2624 - val_acc: 0.2840\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 121us/step - loss: 1.5060 - acc: 0.4768 - val_loss: 2.2518 - val_acc: 0.2900\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 1.4873 - acc: 0.4804 - val_loss: 2.2401 - val_acc: 0.2900\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 1.4700 - acc: 0.4912 - val_loss: 2.2295 - val_acc: 0.2840\n",
      "Numbers of exp: 1, reduce_factor: 0.20, reduce_patient: 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 1s 508us/step - loss: 3.0265 - acc: 0.1052 - val_loss: 3.0764 - val_acc: 0.1020\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.9563 - acc: 0.1128 - val_loss: 2.9989 - val_acc: 0.1080\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 2.8535 - acc: 0.1304 - val_loss: 2.9294 - val_acc: 0.1260\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 2.7429 - acc: 0.1424 - val_loss: 2.8702 - val_acc: 0.1420\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 2.6446 - acc: 0.1568 - val_loss: 2.8274 - val_acc: 0.1580\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.5452 - acc: 0.1860 - val_loss: 2.7963 - val_acc: 0.1720\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.4586 - acc: 0.1984 - val_loss: 2.7678 - val_acc: 0.1820\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 86us/step - loss: 2.3800 - acc: 0.2204 - val_loss: 2.7320 - val_acc: 0.2000\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.3087 - acc: 0.2296 - val_loss: 2.6906 - val_acc: 0.1980\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 2.2454 - acc: 0.2532 - val_loss: 2.6447 - val_acc: 0.2080\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.1836 - acc: 0.2648 - val_loss: 2.6032 - val_acc: 0.2020\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.1262 - acc: 0.2820 - val_loss: 2.5627 - val_acc: 0.2180\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 92us/step - loss: 2.0748 - acc: 0.2892 - val_loss: 2.5246 - val_acc: 0.2200\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 2.0322 - acc: 0.3072 - val_loss: 2.4923 - val_acc: 0.2200\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 1.9819 - acc: 0.3228 - val_loss: 2.4642 - val_acc: 0.2140\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 1.9365 - acc: 0.3440 - val_loss: 2.4410 - val_acc: 0.2100\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 1.8990 - acc: 0.3488 - val_loss: 2.4186 - val_acc: 0.2120\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 1.8630 - acc: 0.3644 - val_loss: 2.3993 - val_acc: 0.2200\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 1.8245 - acc: 0.3736 - val_loss: 2.3808 - val_acc: 0.2160\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 1.7923 - acc: 0.3824 - val_loss: 2.3647 - val_acc: 0.2280\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 1.7608 - acc: 0.3904 - val_loss: 2.3509 - val_acc: 0.2300\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 1.7269 - acc: 0.4100 - val_loss: 2.3392 - val_acc: 0.2360\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 1.6996 - acc: 0.4172 - val_loss: 2.3283 - val_acc: 0.2340\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 1.6722 - acc: 0.4244 - val_loss: 2.3169 - val_acc: 0.2300\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 1.6479 - acc: 0.4376 - val_loss: 2.3033 - val_acc: 0.2320\n",
      "Numbers of exp: 2, reduce_factor: 0.50, reduce_patient: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 1s 479us/step - loss: 3.1548 - acc: 0.0952 - val_loss: 3.1345 - val_acc: 0.0880\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 3.0679 - acc: 0.0996 - val_loss: 3.0476 - val_acc: 0.0960\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 2.9384 - acc: 0.1064 - val_loss: 2.9761 - val_acc: 0.1240\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.7952 - acc: 0.1208 - val_loss: 2.9460 - val_acc: 0.1400\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.6664 - acc: 0.1408 - val_loss: 2.9616 - val_acc: 0.1360\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 2.5537 - acc: 0.1624 - val_loss: 2.9960 - val_acc: 0.1360\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 2.4540 - acc: 0.1824 - val_loss: 3.0181 - val_acc: 0.1440\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 2.3738 - acc: 0.2152 - val_loss: 3.0250 - val_acc: 0.1440\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 2.3083 - acc: 0.2304 - val_loss: 3.0189 - val_acc: 0.1540\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.2628 - acc: 0.2432 - val_loss: 2.9982 - val_acc: 0.1540\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.2158 - acc: 0.2536 - val_loss: 2.9729 - val_acc: 0.1560\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 88us/step - loss: 2.1809 - acc: 0.2620 - val_loss: 2.9424 - val_acc: 0.1620\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.1544 - acc: 0.2728 - val_loss: 2.9070 - val_acc: 0.1600\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1266 - acc: 0.2868 - val_loss: 2.8644 - val_acc: 0.1620\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.0991 - acc: 0.2944 - val_loss: 2.8207 - val_acc: 0.1640\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 2.0735 - acc: 0.2980 - val_loss: 2.7763 - val_acc: 0.1700\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.0551 - acc: 0.3080 - val_loss: 2.7346 - val_acc: 0.1740\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.0340 - acc: 0.3116 - val_loss: 2.6939 - val_acc: 0.1800\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.0130 - acc: 0.3176 - val_loss: 2.6534 - val_acc: 0.1860\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 1.9921 - acc: 0.3232 - val_loss: 2.6155 - val_acc: 0.1940\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 1.9763 - acc: 0.3288 - val_loss: 2.5817 - val_acc: 0.2060\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 1.9563 - acc: 0.3372 - val_loss: 2.5510 - val_acc: 0.2060\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 1.9436 - acc: 0.3372 - val_loss: 2.5218 - val_acc: 0.2120\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 1.9263 - acc: 0.3412 - val_loss: 2.4948 - val_acc: 0.2220\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 92us/step - loss: 1.9164 - acc: 0.3404 - val_loss: 2.4707 - val_acc: 0.2240\n",
      "Numbers of exp: 3, reduce_factor: 0.50, reduce_patient: 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 1s 534us/step - loss: 3.0067 - acc: 0.1088 - val_loss: 3.0452 - val_acc: 0.1140\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.9980 - acc: 0.1072 - val_loss: 3.0245 - val_acc: 0.1180\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 2.9705 - acc: 0.1140 - val_loss: 2.9972 - val_acc: 0.1240\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.9352 - acc: 0.1108 - val_loss: 2.9687 - val_acc: 0.1220\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.8963 - acc: 0.1148 - val_loss: 2.9350 - val_acc: 0.1260\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 2.8567 - acc: 0.1200 - val_loss: 2.9015 - val_acc: 0.1300\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.8116 - acc: 0.1328 - val_loss: 2.8731 - val_acc: 0.1320\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 2.7714 - acc: 0.1364 - val_loss: 2.8470 - val_acc: 0.1440\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.7255 - acc: 0.1408 - val_loss: 2.8251 - val_acc: 0.1500\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.6860 - acc: 0.1520 - val_loss: 2.8063 - val_acc: 0.1520\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 2.6405 - acc: 0.1604 - val_loss: 2.7925 - val_acc: 0.1500\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 2.5968 - acc: 0.1672 - val_loss: 2.7805 - val_acc: 0.1480\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.5640 - acc: 0.1736 - val_loss: 2.7706 - val_acc: 0.1500\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.5278 - acc: 0.1856 - val_loss: 2.7608 - val_acc: 0.1600\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 2.4944 - acc: 0.1956 - val_loss: 2.7500 - val_acc: 0.1580\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.4628 - acc: 0.1992 - val_loss: 2.7385 - val_acc: 0.1640\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 2.4350 - acc: 0.2036 - val_loss: 2.7251 - val_acc: 0.1680\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 90us/step - loss: 2.4043 - acc: 0.2108 - val_loss: 2.7113 - val_acc: 0.1660\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.3775 - acc: 0.2124 - val_loss: 2.6988 - val_acc: 0.1740\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.3489 - acc: 0.2208 - val_loss: 2.6853 - val_acc: 0.1760\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.3272 - acc: 0.2256 - val_loss: 2.6739 - val_acc: 0.1740\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.3002 - acc: 0.2284 - val_loss: 2.6610 - val_acc: 0.1820\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.2776 - acc: 0.2392 - val_loss: 2.6471 - val_acc: 0.1840\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 89us/step - loss: 2.2542 - acc: 0.2408 - val_loss: 2.6322 - val_acc: 0.1840\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 2.2332 - acc: 0.2476 - val_loss: 2.6161 - val_acc: 0.1820\n",
      "Numbers of exp: 4, reduce_factor: 0.20, reduce_patient: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 631us/step - loss: 2.5419 - acc: 0.1888 - val_loss: 5.4222 - val_acc: 0.1840\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 1.8521 - acc: 0.3644 - val_loss: 3.6410 - val_acc: 0.1720\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 1.5796 - acc: 0.4460 - val_loss: 2.7725 - val_acc: 0.2420\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 1.3907 - acc: 0.5296 - val_loss: 2.4351 - val_acc: 0.3080\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 1.2579 - acc: 0.5856 - val_loss: 2.4486 - val_acc: 0.2960\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 1.1295 - acc: 0.6344 - val_loss: 2.3405 - val_acc: 0.3320\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.0132 - acc: 0.6852 - val_loss: 2.1857 - val_acc: 0.3480\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 0.9167 - acc: 0.7256 - val_loss: 2.2284 - val_acc: 0.3360\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 0.8097 - acc: 0.7716 - val_loss: 2.2994 - val_acc: 0.3360\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 0.7152 - acc: 0.8120 - val_loss: 2.2418 - val_acc: 0.3460\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 0.6311 - acc: 0.8516 - val_loss: 2.1366 - val_acc: 0.3500\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 0.5957 - acc: 0.8700 - val_loss: 2.0399 - val_acc: 0.3880\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 0.5721 - acc: 0.8796 - val_loss: 2.0141 - val_acc: 0.4120\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 0.5480 - acc: 0.8888 - val_loss: 1.9817 - val_acc: 0.4140\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 0.5200 - acc: 0.8960 - val_loss: 1.9659 - val_acc: 0.4060\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 0.4974 - acc: 0.9076 - val_loss: 1.9754 - val_acc: 0.4000\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 91us/step - loss: 0.4725 - acc: 0.9176 - val_loss: 1.9662 - val_acc: 0.4060\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 0.4508 - acc: 0.9216 - val_loss: 1.9439 - val_acc: 0.4080\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 0.4342 - acc: 0.9288 - val_loss: 1.9356 - val_acc: 0.4200\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 0.4118 - acc: 0.9416 - val_loss: 1.9445 - val_acc: 0.4080\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 0.3939 - acc: 0.9504 - val_loss: 1.9443 - val_acc: 0.3940\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 0.3712 - acc: 0.9572 - val_loss: 1.9448 - val_acc: 0.4080\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 0.3574 - acc: 0.9568 - val_loss: 1.9408 - val_acc: 0.4180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 0.3515 - acc: 0.9612 - val_loss: 1.9367 - val_acc: 0.4200\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 0.3477 - acc: 0.9632 - val_loss: 1.9314 - val_acc: 0.4120\n",
      "Numbers of exp: 5, reduce_factor: 0.20, reduce_patient: 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 731us/step - loss: 3.0061 - acc: 0.1060 - val_loss: 2.9674 - val_acc: 0.1180\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 106us/step - loss: 2.6174 - acc: 0.1516 - val_loss: 2.8900 - val_acc: 0.1520\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 2.3401 - acc: 0.2068 - val_loss: 2.7908 - val_acc: 0.1780\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1414 - acc: 0.2752 - val_loss: 2.6826 - val_acc: 0.2020\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 105us/step - loss: 2.0008 - acc: 0.3160 - val_loss: 2.5791 - val_acc: 0.2260\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 1.8779 - acc: 0.3508 - val_loss: 2.4624 - val_acc: 0.2440\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 1.7759 - acc: 0.3804 - val_loss: 2.3905 - val_acc: 0.2640\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.6880 - acc: 0.4024 - val_loss: 2.3559 - val_acc: 0.2800\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 1.6172 - acc: 0.4260 - val_loss: 2.3193 - val_acc: 0.2780\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 1.5491 - acc: 0.4580 - val_loss: 2.2658 - val_acc: 0.2880\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.4864 - acc: 0.4784 - val_loss: 2.2052 - val_acc: 0.2880\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 1.4295 - acc: 0.5112 - val_loss: 2.1681 - val_acc: 0.3100\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 1.3804 - acc: 0.5348 - val_loss: 2.1401 - val_acc: 0.2920\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 111us/step - loss: 1.3334 - acc: 0.5428 - val_loss: 2.1316 - val_acc: 0.2900\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.2902 - acc: 0.5756 - val_loss: 2.1252 - val_acc: 0.2940\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 1.2527 - acc: 0.5968 - val_loss: 2.1164 - val_acc: 0.2960\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 110us/step - loss: 1.2134 - acc: 0.6132 - val_loss: 2.1115 - val_acc: 0.3060\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 1.1811 - acc: 0.6308 - val_loss: 2.1109 - val_acc: 0.3020\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.1483 - acc: 0.6476 - val_loss: 2.1013 - val_acc: 0.3060\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.1160 - acc: 0.6648 - val_loss: 2.0858 - val_acc: 0.3040\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.0889 - acc: 0.6784 - val_loss: 2.0658 - val_acc: 0.3060\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.0597 - acc: 0.6968 - val_loss: 2.0573 - val_acc: 0.3100\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.0336 - acc: 0.7160 - val_loss: 2.0510 - val_acc: 0.3120\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 109us/step - loss: 1.0123 - acc: 0.7228 - val_loss: 2.0503 - val_acc: 0.2960\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 0.9889 - acc: 0.7336 - val_loss: 2.0491 - val_acc: 0.3000\n",
      "Numbers of exp: 6, reduce_factor: 0.50, reduce_patient: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 753us/step - loss: 2.9723 - acc: 0.1144 - val_loss: 2.9884 - val_acc: 0.1500\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 2.4931 - acc: 0.2032 - val_loss: 2.9968 - val_acc: 0.1800\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 2.2254 - acc: 0.2688 - val_loss: 2.7746 - val_acc: 0.2220\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 2.0370 - acc: 0.3292 - val_loss: 2.6003 - val_acc: 0.2400\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 106us/step - loss: 1.8950 - acc: 0.3652 - val_loss: 2.4934 - val_acc: 0.2520\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 1.7568 - acc: 0.3980 - val_loss: 2.3976 - val_acc: 0.2560\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 1.6530 - acc: 0.4404 - val_loss: 2.3505 - val_acc: 0.2600\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 1.5605 - acc: 0.4676 - val_loss: 2.3064 - val_acc: 0.2900\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.4805 - acc: 0.4996 - val_loss: 2.2688 - val_acc: 0.2940\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.4062 - acc: 0.5328 - val_loss: 2.2242 - val_acc: 0.2820\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 1.3458 - acc: 0.5512 - val_loss: 2.1901 - val_acc: 0.2820\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.2900 - acc: 0.5716 - val_loss: 2.1649 - val_acc: 0.2940\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 1.2340 - acc: 0.5960 - val_loss: 2.1526 - val_acc: 0.2960\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 1.1849 - acc: 0.6188 - val_loss: 2.1463 - val_acc: 0.3020\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 1.1436 - acc: 0.6368 - val_loss: 2.1304 - val_acc: 0.3080\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 1.1024 - acc: 0.6604 - val_loss: 2.1049 - val_acc: 0.3000\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.0650 - acc: 0.6796 - val_loss: 2.0834 - val_acc: 0.3020\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 118us/step - loss: 1.0301 - acc: 0.6956 - val_loss: 2.0779 - val_acc: 0.3020\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 0.9989 - acc: 0.7132 - val_loss: 2.0838 - val_acc: 0.3020\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 0.9695 - acc: 0.7304 - val_loss: 2.0892 - val_acc: 0.2980\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 0.9396 - acc: 0.7452 - val_loss: 2.0847 - val_acc: 0.3060\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 2.0000001313746907e-05.\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 0.9135 - acc: 0.7536 - val_loss: 2.0726 - val_acc: 0.3080\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 0.8973 - acc: 0.7660 - val_loss: 2.0627 - val_acc: 0.3200\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 0.8865 - acc: 0.7720 - val_loss: 2.0531 - val_acc: 0.3280\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 0.8705 - acc: 0.7784 - val_loss: 2.0491 - val_acc: 0.3140\n",
      "Numbers of exp: 7, reduce_factor: 0.50, reduce_patient: 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 695us/step - loss: 3.0991 - acc: 0.0984 - val_loss: 2.8589 - val_acc: 0.1480\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 2.7380 - acc: 0.1392 - val_loss: 2.7638 - val_acc: 0.1900\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.4495 - acc: 0.1916 - val_loss: 2.7299 - val_acc: 0.2000\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 2.2561 - acc: 0.2360 - val_loss: 2.6652 - val_acc: 0.2260\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 2.1203 - acc: 0.2772 - val_loss: 2.5591 - val_acc: 0.2420\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.0005 - acc: 0.3140 - val_loss: 2.4164 - val_acc: 0.2620\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.8979 - acc: 0.3440 - val_loss: 2.3276 - val_acc: 0.2780\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 1.8089 - acc: 0.3620 - val_loss: 2.2662 - val_acc: 0.2940\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 105us/step - loss: 1.7332 - acc: 0.3932 - val_loss: 2.2408 - val_acc: 0.2860\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 105us/step - loss: 1.6659 - acc: 0.4156 - val_loss: 2.2120 - val_acc: 0.2860\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 149us/step - loss: 1.6064 - acc: 0.4380 - val_loss: 2.1866 - val_acc: 0.3000\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 111us/step - loss: 1.5528 - acc: 0.4548 - val_loss: 2.1658 - val_acc: 0.3120\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 105us/step - loss: 1.5103 - acc: 0.4816 - val_loss: 2.1388 - val_acc: 0.3060\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 1.4626 - acc: 0.4872 - val_loss: 2.1245 - val_acc: 0.3120\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.4251 - acc: 0.5076 - val_loss: 2.1065 - val_acc: 0.3280\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 1.3852 - acc: 0.5260 - val_loss: 2.0912 - val_acc: 0.3180\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 1.3509 - acc: 0.5492 - val_loss: 2.0811 - val_acc: 0.3200\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 1.3221 - acc: 0.5656 - val_loss: 2.0733 - val_acc: 0.3260\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 111us/step - loss: 1.2908 - acc: 0.5872 - val_loss: 2.0727 - val_acc: 0.3300\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 1.2634 - acc: 0.5920 - val_loss: 2.0715 - val_acc: 0.3260\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.2383 - acc: 0.6024 - val_loss: 2.0609 - val_acc: 0.3240\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 1.2102 - acc: 0.6184 - val_loss: 2.0524 - val_acc: 0.3280\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 103us/step - loss: 1.1855 - acc: 0.6368 - val_loss: 2.0514 - val_acc: 0.3280\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 1.1693 - acc: 0.6448 - val_loss: 2.0493 - val_acc: 0.3240\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 105us/step - loss: 1.1430 - acc: 0.6548 - val_loss: 2.0403 - val_acc: 0.3240\n",
      "Numbers of exp: 8, reduce_factor: 0.20, reduce_patient: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 715us/step - loss: 2.7066 - acc: 0.1752 - val_loss: 14.1354 - val_acc: 0.1100\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.2033 - acc: 0.2972 - val_loss: 4.5589 - val_acc: 0.1580\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.8542 - acc: 0.3676 - val_loss: 2.3636 - val_acc: 0.2400\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 1.6122 - acc: 0.4432 - val_loss: 2.7518 - val_acc: 0.2180\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.4965 - acc: 0.4764 - val_loss: 2.3731 - val_acc: 0.2340\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 113us/step - loss: 1.3532 - acc: 0.5348 - val_loss: 3.4447 - val_acc: 0.1840\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 104us/step - loss: 1.2417 - acc: 0.5952 - val_loss: 2.2126 - val_acc: 0.2520\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 1.0838 - acc: 0.6612 - val_loss: 2.0569 - val_acc: 0.2780\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 1.0136 - acc: 0.7040 - val_loss: 1.9989 - val_acc: 0.2920\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 0.9638 - acc: 0.7224 - val_loss: 2.0243 - val_acc: 0.3000\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 127us/step - loss: 0.9137 - acc: 0.7460 - val_loss: 2.0111 - val_acc: 0.2860\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 116us/step - loss: 0.8782 - acc: 0.7656 - val_loss: 2.1529 - val_acc: 0.2680\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 102us/step - loss: 0.8323 - acc: 0.7896 - val_loss: 2.0535 - val_acc: 0.2860\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 0.8083 - acc: 0.8016 - val_loss: 2.0012 - val_acc: 0.3020\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 0.7970 - acc: 0.8092 - val_loss: 1.9906 - val_acc: 0.3100\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 0.7874 - acc: 0.8100 - val_loss: 1.9706 - val_acc: 0.3160\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 108us/step - loss: 0.7748 - acc: 0.8116 - val_loss: 1.9670 - val_acc: 0.3320\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 112us/step - loss: 0.7647 - acc: 0.8156 - val_loss: 1.9598 - val_acc: 0.3400\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 0.7535 - acc: 0.8244 - val_loss: 1.9561 - val_acc: 0.3300\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 107us/step - loss: 0.7447 - acc: 0.8244 - val_loss: 1.9384 - val_acc: 0.3400\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 133us/step - loss: 0.7389 - acc: 0.8276 - val_loss: 1.9415 - val_acc: 0.3360\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 0.7278 - acc: 0.8320 - val_loss: 1.9252 - val_acc: 0.3420\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 0.7174 - acc: 0.8404 - val_loss: 1.9445 - val_acc: 0.3400\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 101us/step - loss: 0.7094 - acc: 0.8420 - val_loss: 1.9297 - val_acc: 0.3440\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 1s 257us/step - loss: 0.7012 - acc: 0.8412 - val_loss: 1.9291 - val_acc: 0.3420\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Numbers of exp: 9, reduce_factor: 0.20, reduce_patient: 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 794us/step - loss: 2.9721 - acc: 0.1020 - val_loss: 2.8490 - val_acc: 0.1180\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.8356 - acc: 0.1180 - val_loss: 2.8081 - val_acc: 0.1180\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.7552 - acc: 0.1324 - val_loss: 2.7696 - val_acc: 0.1340\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.6893 - acc: 0.1488 - val_loss: 2.7478 - val_acc: 0.1500\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.6388 - acc: 0.1560 - val_loss: 2.7244 - val_acc: 0.1580\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.5889 - acc: 0.1552 - val_loss: 2.6918 - val_acc: 0.1660\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.5477 - acc: 0.1700 - val_loss: 2.6705 - val_acc: 0.1740\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.5129 - acc: 0.1768 - val_loss: 2.6431 - val_acc: 0.1800\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.4768 - acc: 0.1864 - val_loss: 2.6345 - val_acc: 0.1820\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.4393 - acc: 0.1908 - val_loss: 2.6084 - val_acc: 0.1860\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.4117 - acc: 0.1968 - val_loss: 2.5914 - val_acc: 0.1900\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.3761 - acc: 0.2048 - val_loss: 2.5822 - val_acc: 0.1940\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 2.3525 - acc: 0.2200 - val_loss: 2.5616 - val_acc: 0.1960\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.3261 - acc: 0.2224 - val_loss: 2.5480 - val_acc: 0.2000\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.3001 - acc: 0.2268 - val_loss: 2.5358 - val_acc: 0.2020\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.2695 - acc: 0.2332 - val_loss: 2.5158 - val_acc: 0.2140\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.2458 - acc: 0.2368 - val_loss: 2.5040 - val_acc: 0.2200\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.2232 - acc: 0.2476 - val_loss: 2.4913 - val_acc: 0.2180\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.1984 - acc: 0.2484 - val_loss: 2.4824 - val_acc: 0.2200\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1734 - acc: 0.2508 - val_loss: 2.4686 - val_acc: 0.2200\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 2.1562 - acc: 0.2612 - val_loss: 2.4577 - val_acc: 0.2100\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1308 - acc: 0.2648 - val_loss: 2.4494 - val_acc: 0.2200\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.1152 - acc: 0.2704 - val_loss: 2.4399 - val_acc: 0.2160\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.0925 - acc: 0.2792 - val_loss: 2.4275 - val_acc: 0.2200\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.0776 - acc: 0.2800 - val_loss: 2.4247 - val_acc: 0.2200\n",
      "Numbers of exp: 10, reduce_factor: 0.50, reduce_patient: 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 783us/step - loss: 2.9713 - acc: 0.1060 - val_loss: 2.9728 - val_acc: 0.1120\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.8158 - acc: 0.1220 - val_loss: 2.9169 - val_acc: 0.1180\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.7231 - acc: 0.1432 - val_loss: 2.8678 - val_acc: 0.1280\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.6547 - acc: 0.1568 - val_loss: 2.8316 - val_acc: 0.1440\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.5972 - acc: 0.1644 - val_loss: 2.8074 - val_acc: 0.1540\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 2.5475 - acc: 0.1724 - val_loss: 2.7850 - val_acc: 0.1660\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.5083 - acc: 0.1840 - val_loss: 2.7646 - val_acc: 0.1720\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.4651 - acc: 0.1920 - val_loss: 2.7370 - val_acc: 0.1860\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.4276 - acc: 0.2020 - val_loss: 2.7254 - val_acc: 0.1920\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.3877 - acc: 0.2184 - val_loss: 2.7044 - val_acc: 0.1940\n",
      "Epoch 11/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.3522 - acc: 0.2216 - val_loss: 2.6931 - val_acc: 0.2000\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.3173 - acc: 0.2348 - val_loss: 2.6839 - val_acc: 0.2020\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.2935 - acc: 0.2400 - val_loss: 2.6706 - val_acc: 0.2060\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.2634 - acc: 0.2460 - val_loss: 2.6574 - val_acc: 0.2040\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.2313 - acc: 0.2624 - val_loss: 2.6459 - val_acc: 0.2100\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.2119 - acc: 0.2616 - val_loss: 2.6402 - val_acc: 0.2040\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 2.1860 - acc: 0.2724 - val_loss: 2.6319 - val_acc: 0.2060\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.1587 - acc: 0.2768 - val_loss: 2.6120 - val_acc: 0.2060\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1375 - acc: 0.2756 - val_loss: 2.5995 - val_acc: 0.2040\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 98us/step - loss: 2.1204 - acc: 0.2856 - val_loss: 2.5869 - val_acc: 0.2080\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.0937 - acc: 0.2908 - val_loss: 2.5652 - val_acc: 0.2160\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.0736 - acc: 0.2940 - val_loss: 2.5576 - val_acc: 0.2140\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 93us/step - loss: 2.0532 - acc: 0.3012 - val_loss: 2.5383 - val_acc: 0.2160\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.0294 - acc: 0.3036 - val_loss: 2.5298 - val_acc: 0.2220\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.0218 - acc: 0.3116 - val_loss: 2.5126 - val_acc: 0.2200\n",
      "Numbers of exp: 11, reduce_factor: 0.50, reduce_patient: 5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "hidden_layer1 (Dense)        (None, 256)               786688    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer2 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "hidden_layer3 (Dense)        (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 922,378\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Train on 2500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "2500/2500 [==============================] - 2s 862us/step - loss: 3.0522 - acc: 0.1008 - val_loss: 2.9241 - val_acc: 0.1060\n",
      "Epoch 2/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.9044 - acc: 0.1176 - val_loss: 2.8621 - val_acc: 0.1300\n",
      "Epoch 3/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.8209 - acc: 0.1284 - val_loss: 2.8226 - val_acc: 0.1300\n",
      "Epoch 4/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.7540 - acc: 0.1348 - val_loss: 2.7803 - val_acc: 0.1420\n",
      "Epoch 5/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 2.6972 - acc: 0.1516 - val_loss: 2.7543 - val_acc: 0.1520\n",
      "Epoch 6/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.6515 - acc: 0.1616 - val_loss: 2.7283 - val_acc: 0.1540\n",
      "Epoch 7/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.6025 - acc: 0.1680 - val_loss: 2.7074 - val_acc: 0.1620\n",
      "Epoch 8/25\n",
      "2500/2500 [==============================] - 0s 99us/step - loss: 2.5629 - acc: 0.1788 - val_loss: 2.6805 - val_acc: 0.1600\n",
      "Epoch 9/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.5207 - acc: 0.1848 - val_loss: 2.6664 - val_acc: 0.1640\n",
      "Epoch 10/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.4859 - acc: 0.1932 - val_loss: 2.6519 - val_acc: 0.1700\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.4528 - acc: 0.2008 - val_loss: 2.6404 - val_acc: 0.1740\n",
      "Epoch 12/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.4163 - acc: 0.2116 - val_loss: 2.6227 - val_acc: 0.1800\n",
      "Epoch 13/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.3879 - acc: 0.2136 - val_loss: 2.6013 - val_acc: 0.1760\n",
      "Epoch 14/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.3563 - acc: 0.2232 - val_loss: 2.5871 - val_acc: 0.1800\n",
      "Epoch 15/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.3258 - acc: 0.2296 - val_loss: 2.5703 - val_acc: 0.1860\n",
      "Epoch 16/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.2992 - acc: 0.2340 - val_loss: 2.5589 - val_acc: 0.1920\n",
      "Epoch 17/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.2727 - acc: 0.2448 - val_loss: 2.5409 - val_acc: 0.1880\n",
      "Epoch 18/25\n",
      "2500/2500 [==============================] - 0s 100us/step - loss: 2.2510 - acc: 0.2468 - val_loss: 2.5259 - val_acc: 0.2020\n",
      "Epoch 19/25\n",
      "2500/2500 [==============================] - 0s 94us/step - loss: 2.2230 - acc: 0.2540 - val_loss: 2.5189 - val_acc: 0.2000\n",
      "Epoch 20/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1998 - acc: 0.2608 - val_loss: 2.5150 - val_acc: 0.2000\n",
      "Epoch 21/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1785 - acc: 0.2684 - val_loss: 2.5042 - val_acc: 0.2040\n",
      "Epoch 22/25\n",
      "2500/2500 [==============================] - 0s 97us/step - loss: 2.1595 - acc: 0.2772 - val_loss: 2.4928 - val_acc: 0.2040\n",
      "Epoch 23/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.1375 - acc: 0.2808 - val_loss: 2.4782 - val_acc: 0.2040\n",
      "Epoch 24/25\n",
      "2500/2500 [==============================] - 0s 96us/step - loss: 2.1162 - acc: 0.2848 - val_loss: 2.4615 - val_acc: 0.2040\n",
      "Epoch 25/25\n",
      "2500/2500 [==============================] - 0s 95us/step - loss: 2.0925 - acc: 0.2884 - val_loss: 2.4488 - val_acc: 0.2100\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "results = {}\n",
    "for i, (optim, reduce_factor, reduce_patient) in enumerate(itertools.product(optimizer_set, reduce_lr_factor, redice_lr_patient)):\n",
    "    print(\"Numbers of exp: %i, reduce_factor: %.2f, reduce_patient: %i\" % (i+1, reduce_factor, reduce_patient))\n",
    "    model = build_mlp(input_shape=x_train.shape[1:])\n",
    "    model.summary()\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optim)\n",
    "    \n",
    "    \"\"\"Code Here\n",
    "    設定 reduce learning rate 的 callback function\n",
    "    \"\"\"\n",
    "    reduce_lr = ReduceLROnPlateau(factor=reduce_factor, \n",
    "                              min_lr=1e-12, \n",
    "                              monitor='val_loss', \n",
    "                              patience=reduce_patient, \n",
    "                              verbose=1)\n",
    "    \n",
    "    model.fit(x_train, y_train, \n",
    "              epochs=EPOCHS, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              validation_data=(x_test, y_test), \n",
    "              shuffle=True,\n",
    "              callbacks=[reduce_lr]\n",
    "             )\n",
    "\n",
    "    # Collect results\n",
    "    exp_name_tag = (\"exp-%i\" % (i+1))\n",
    "    results[exp_name_tag] = {'train-loss': model.history.history[\"loss\"],\n",
    "                             'valid-loss': model.history.history[\"val_loss\"],\n",
    "                             'train-acc': model.history.history[\"acc\"],\n",
    "                             'valid-acc': model.history.history[\"val_acc\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "NUM_COLORS = len(results.keys())\n",
    "\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=NUM_COLORS-1)\n",
    "scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "color_bar = [scalarMap.to_rgba(i) for i in range(NUM_COLORS)]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train-loss'])),results[cond]['train-loss'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid-loss'])),results[cond]['valid-loss'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Loss\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, cond in enumerate(results.keys()):\n",
    "    plt.plot(range(len(results[cond]['train-acc'])),results[cond]['train-acc'], '-', label=cond, color=color_bar[i])\n",
    "    plt.plot(range(len(results[cond]['valid-acc'])),results[cond]['valid-acc'], '--', label=cond, color=color_bar[i])\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
